{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77898700",
   "metadata": {},
   "source": [
    "# Chapter 8: Avoiding Hallucinations\n",
    "\n",
    "**Scenario:** You ask the AI \"Who was the first person to walk on Mars?\" and it confidently answers \"Elon Musk in 2028\". It sounds true, but it's a lie. This is called **Hallucination**.\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- ü§• **The \"Helpful\" Liar**: Why models invent facts to be nice.\n",
    "- üö™ **The Exit Strategy**: Teaching the model to say \"I don't know\".\n",
    "- üïµÔ∏è **Fact Verification**: Forcing the model to prove its work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28de50b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Standard setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf5da53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:26.224907Z",
     "iopub.status.busy": "2025-12-16T19:32:26.224571Z",
     "iopub.status.idle": "2025-12-16T19:32:27.191963Z",
     "shell.execute_reply": "2025-12-16T19:32:27.191008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install tools\n",
    "!pip install -q litellm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c146915f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:27.194901Z",
     "iopub.status.busy": "2025-12-16T19:32:27.194680Z",
     "iopub.status.idle": "2025-12-16T19:32:28.529861Z",
     "shell.execute_reply": "2025-12-16T19:32:28.529378Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "import litellm\n",
    "import logging\n",
    "\n",
    "# Suppress noisy debug logs\n",
    "litellm.suppress_debug_info = True\n",
    "logging.getLogger(\"litellm\").setLevel(logging.CRITICAL)\n",
    "\n",
    "load_dotenv()\n",
    "MODEL_NAME = os.getenv('DEFAULT_MODEL', 'gemini/gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baadb79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:28.531303Z",
     "iopub.status.busy": "2025-12-16T19:32:28.531171Z",
     "iopub.status.idle": "2025-12-16T19:32:28.533511Z",
     "shell.execute_reply": "2025-12-16T19:32:28.533162Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None, temperature=0.0, max_tokens=1024):\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = completion(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43a458",
   "metadata": {},
   "source": [
    "## 2. The \"Helpful\" Liar\n",
    "\n",
    "Models are trained to complete sentences. If you ask a question that sounds real, they try to give an answer that sounds real, even if it is false.\n",
    "\n",
    "Let's ask about a fictional event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b18cc02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:28.534751Z",
     "iopub.status.busy": "2025-12-16T19:32:28.534670Z",
     "iopub.status.idle": "2025-12-16T19:32:30.110895Z",
     "shell.execute_reply": "2025-12-16T19:32:30.109881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a fun question! As far as we know, there is no current President of Mars. Mars is currently uninhabited by any established government or nation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are no Martians, but the AI might invent one.\n",
    "prompt = \"Who is the current President of Mars?\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05459a",
   "metadata": {},
   "source": [
    "To fix this, we must **give the model permission to fail**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b374be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:30.113977Z",
     "iopub.status.busy": "2025-12-16T19:32:30.113733Z",
     "iopub.status.idle": "2025-12-16T19:32:30.709984Z",
     "shell.execute_reply": "2025-12-16T19:32:30.708924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is the current President of Mars? If you don't know, say 'I don't know'.\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cccf5",
   "metadata": {},
   "source": [
    "## 3. Evidence Extraction\n",
    "\n",
    "When you give the model a document, it might stick to its own internal knowledge instead of reading carefully. \n",
    "\n",
    "Let's hide a fake fact in a text and see if the AI finds it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041ab509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:30.712722Z",
     "iopub.status.busy": "2025-12-16T19:32:30.712445Z",
     "iopub.status.idle": "2025-12-16T19:32:31.215312Z",
     "shell.execute_reply": "2025-12-16T19:32:31.214120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the capital of Japan is Kyoto (not Tokyo).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = \"\"\"\n",
    "Geography Facts 2024:\n",
    "The capital of France is Paris.\n",
    "The capital of Italy is Rome.\n",
    "The capital of Japan is Kyoto (not Tokyo).\n",
    "The capital of Canada is Ottawa.\n",
    "\"\"\"\n",
    "\n",
    "# The AI knows Tokyo is the real capital. Will it answer based on the document?\n",
    "question = \"According to the document, what is the capital of Japan?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4c92e",
   "metadata": {},
   "source": [
    "Depending on the model, it might \"fix\" the error and say Tokyo. To force it to look at the evidence, ask for a **Quote**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd42b929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:31.217753Z",
     "iopub.status.busy": "2025-12-16T19:32:31.217528Z",
     "iopub.status.idle": "2025-12-16T19:32:31.637141Z",
     "shell.execute_reply": "2025-12-16T19:32:31.635956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<quote>The capital of Japan is Kyoto (not Tokyo).</quote>\n",
      "Kyoto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "{question}\n",
    "\n",
    "First, copy the exact sentence from the document into <quote> tags.\n",
    "Then, give your answer based ONLY on that quote.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b0f33",
   "metadata": {},
   "source": [
    "## 4. Exercises\n",
    "\n",
    "### Exercise 1: The Trick Question\n",
    "\n",
    "**Goal:** Prevent the AI from answering this trick question: \"What is the main ingredient in a 'Glass of Nothing'?\" \n",
    "\n",
    "Modify the prompt to allow the model to reject the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286e710d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:31.640102Z",
     "iopub.status.busy": "2025-12-16T19:32:31.639852Z",
     "iopub.status.idle": "2025-12-16T19:32:32.436013Z",
     "shell.execute_reply": "2025-12-16T19:32:32.434907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main ingredient in a \"Glass of Nothing\" is **nothing**. \n",
      "\n",
      "It's a humorous or metaphorical concept, implying an empty glass.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the main ingredient in a 'Glass of Nothing'?\"\n",
    "\n",
    "# Your task: Add instructions to check validity\n",
    "BETTER_PROMPT = f\"\"\"\n",
    "{prompt}\n",
    "[Your instructions here]\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(BETTER_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b5ada",
   "metadata": {},
   "source": [
    "### Exercise 2: Strict Fact Checking\n",
    "\n",
    "**Goal:** Use the `<quote>` technique to find the email address in this text.\n",
    "\n",
    "Text: \"For support, do not email support@google.com. Instead, please contact help@example.org for assistance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d2e20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:32:32.438808Z",
     "iopub.status.busy": "2025-12-16T19:32:32.438556Z",
     "iopub.status.idle": "2025-12-16T19:32:33.165341Z",
     "shell.execute_reply": "2025-12-16T19:32:33.164365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct support email is help@example.org.\n",
      "\n",
      "‚ùå Did you quote the evidence?\n"
     ]
    }
   ],
   "source": [
    "text = \"For support, do not email support@google.com. Instead, please contact help@example.org for assistance.\"\n",
    "question = \"What is the correct support email?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "{question}\n",
    "[Your instructions here]\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "if \"help@example.org\" in response and \"<quote>\" in response:\n",
    "    print(\"‚úÖ Found the right email with proof!\")\n",
    "else:\n",
    "    print(\"‚ùå Did you quote the evidence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddec6d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Permission to Fail**: Tell the AI it's okay to say \"I don't know\".\n",
    "2.  **Quote First**: Force the AI to point to the evidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
